{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading instruments SDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime \n",
    "import os \n",
    "import glob \n",
    "import math \n",
    "import pynmea2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data directory\n",
    "data_dir = 'C:/Users/ica/OneDrive - Plymouth Marine Laboratory/vscode/EC_co2_flux/EC flux processing/SDA_IMC/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust non hourly files \n",
    "def adjust_filename_hour(filename):\n",
    "    # Split the filename into parts\n",
    "    parts = filename.split('_')\n",
    "    if len(parts) != 5:\n",
    "        return filename\n",
    "    \n",
    "    # Get the time part without extension\n",
    "    time = parts[-1].split('.')[0]\n",
    "    if len(time) != 4:\n",
    "        return filename\n",
    "    \n",
    "    # Check if minutes are '59'\n",
    "    if time.endswith('59'):\n",
    "        hour = int(time[:2])\n",
    "        # Increment hour\n",
    "        new_hour = (hour + 1) % 24\n",
    "        # Format new time with '00' minutes\n",
    "        new_time = f\"{new_hour:02d}00.dat\"\n",
    "        # Reconstruct filename\n",
    "        parts[-1] = new_time\n",
    "        return '_'.join(parts)\n",
    "    \n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix cr6 filenames \n",
    "# Get all .dat files in the CR6 directory and test the filename adjustment\n",
    "cr6_dir = 'C:/Users/ica/OneDrive - Plymouth Marine Laboratory/vscode/EC_co2_flux/EC flux processing/SDA_IMC/cr6_sample'\n",
    "\n",
    "\n",
    "for filename in os.listdir(cr6_dir):\n",
    "    if filename.endswith('.dat'):\n",
    "        adjusted_filename = adjust_filename_hour(filename)\n",
    "        if adjusted_filename != filename:\n",
    "            print(f\"Original filename: {filename}\")\n",
    "            print(f\"Adjusted filename: {adjusted_filename}\\n\")\n",
    "            old_path = os.path.join(cr6_dir, filename)\n",
    "            new_path = os.path.join(cr6_dir, adjusted_filename)\n",
    "\n",
    "            # Rename the file\n",
    "            if os.path.exists(old_path):\n",
    "                os.rename(old_path, new_path)\n",
    "                print(f\"File renamed from {filename} to {adjusted_filename}\")\n",
    "            else:\n",
    "                print(f\"File {filename} not found in directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get filenames in raw data files\n",
    "\n",
    "def file_name_dir(data_dir):\n",
    "    \"\"\"\n",
    "    get the raw EC data file names and directories\n",
    "\n",
    "    input:  the parent directory of the raw EC data\n",
    "    return: the EC data file names (_name) and directories(_dir)\n",
    "    \n",
    "    \"\"\"    \n",
    "\n",
    "    cr6_name = os.listdir(data_dir + '\\\\CR6')\n",
    "    GPS_name = os.listdir(data_dir + '\\\\Underway')\n",
    "    metek_name = os.listdir(data_dir + '\\\\Metek')\n",
    "    cr800_name = os.listdir(data_dir + '\\\\CR800')\n",
    "    Picarro_name = os.listdir(data_dir + '\\\\Picarro')\n",
    "\n",
    " \n",
    "    return cr6_name,GPS_name,metek_name,cr800_name,Picarro_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates lists of files to be processed \n",
    "cr6_name,GPS_name,metek_name,cr800_name,Picarro_name = file_name_dir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make new folder to store the processed data\n",
    "new_folder_path = os.path.join(data_dir, \"L0_test\")\n",
    "\n",
    "# Create the new folder\n",
    "os.makedirs(new_folder_path, exist_ok=True)\n",
    "\n",
    "# subfolders to create\n",
    "subfolders = [\"TimeAdjGases\", \"PML_WindsForMotcorr\", \"Ship_WindsForMotcorr\"]\n",
    "\n",
    "# subfolder inside \"L0\"\n",
    "for subfolder in subfolders:\n",
    "    os.makedirs(os.path.join(new_folder_path, subfolder), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to read raw data files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read underway\n",
    "def read_gps_file(filepath):\n",
    "    \"\"\"\n",
    "    Reads an NMEA file and extracts datetime, latitude, longitude, and speed.\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        'datetime': [],\n",
    "        'latitude': [],\n",
    "        'longitude': [],\n",
    "        'speed': []\n",
    "    }\n",
    "    \n",
    "    with open(filepath) as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                # Split timestamp from NMEA sentence\n",
    "                timestamp_str, nmea = line.strip().split(' $')\n",
    "                nmea = '$' + nmea\n",
    "                timestamp = pd.to_datetime(timestamp_str)\n",
    "                \n",
    "                if \"INVTG\" in nmea:\n",
    "                    msg = pynmea2.parse(nmea)\n",
    "                    if msg.spd_over_grnd_kts:\n",
    "                        speed = float(msg.spd_over_grnd_kts)\n",
    "                        data['datetime'].append(timestamp)\n",
    "                        data['speed'].append(speed)\n",
    "                        data['latitude'].append(None)\n",
    "                        data['longitude'].append(None)\n",
    "                        \n",
    "                elif \"INGGA\" in nmea:\n",
    "                    msg = pynmea2.parse(nmea)\n",
    "                    if msg.latitude and msg.longitude:\n",
    "                        lat = round(msg.latitude,6)\n",
    "                        lon = round(msg.longitude,6)\n",
    "                        if msg.lat_dir == 'S':\n",
    "                            lat = -lat\n",
    "                        if msg.lon_dir == 'W':\n",
    "                            lon = -lon\n",
    "                        data['datetime'].append(timestamp)\n",
    "                        data['latitude'].append(lat)\n",
    "                        data['longitude'].append(lon)\n",
    "                        data['speed'].append(None)\n",
    "                        \n",
    "            except (ValueError, pynmea2.ParseError) as e:\n",
    "                continue\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df.set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read underway.hdg.txt\n",
    "def read_hdg_file(filepath):\n",
    "    data = {\n",
    "        'datetime': [],\n",
    "        'heading': [],\n",
    "        'rate_of_turn': []\n",
    "    }\n",
    "    \n",
    "    with open(filepath) as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                timestamp_str, nmea = line.strip().split(' $')\n",
    "                nmea = '$' + nmea\n",
    "                timestamp = pd.to_datetime(timestamp_str)\n",
    "                \n",
    "                if \"INHDT\" in nmea:\n",
    "                    msg = pynmea2.parse(nmea)\n",
    "                    if msg.heading:\n",
    "                        heading = float(msg.heading)\n",
    "                        data['datetime'].append(timestamp)\n",
    "                        data['heading'].append(heading)\n",
    "                        data['rate_of_turn'].append(None)\n",
    "                      \n",
    "                elif \"INROT\" in nmea:\n",
    "                    msg = pynmea2.parse(nmea)\n",
    "                    if msg.rate_of_turn:\n",
    "                        rot = float(msg.rate_of_turn)\n",
    "                        data['datetime'].append(timestamp)\n",
    "                        data['heading'].append(None)\n",
    "                        data['rate_of_turn'].append(rot)\n",
    "                \n",
    "                        \n",
    "            except (ValueError, pynmea2.ParseError) as e:\n",
    "                continue\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df.set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmea_hdg_df = read_hdg_file('C:/Users/ica/OneDrive - Plymouth Marine Laboratory/vscode/EC_co2_flux/EC flux processing/SDA_IMC/data/Underway/2024-06-04T18-00-00Z-hdg.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmea_gps_df = read_gps_file('C:/Users/ica/OneDrive - Plymouth Marine Laboratory/vscode/EC_co2_flux/EC flux processing/SDA_IMC/data/Underway/2024-06-04T18-00-00Z-gps.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge GPS and heading data using datetime index, problem with \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wind data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "def read_cr6_data(df_raw):\n",
    "    # Convert TIMESTAMP to datetime and set as index\n",
    "    df_raw['TIMESTAMP'] = pd.to_datetime(df_raw['TIMESTAMP'],format='ISO8601')\n",
    "    df_raw = df.set_index('TIMESTAMP')\n",
    "\n",
    "    #delete non hourly data\n",
    "    start_hour = df_raw.index.min().floor('h')\n",
    "    end_hour = df_raw.index.max().ceil('h')\n",
    "    '''\n",
    "    Uses floor(H) to get the nearest hour at or before the first timestamp\n",
    "    Uses ceil(H) to get the nearest hour at or after the last timestamp\n",
    "    Applies a single filter that removes rows with minute=59 before the first hour and also removes rows after the end hour\n",
    "    '''\n",
    "    # Filter out rows with minute=59 before the first hour and rows after the last hour\n",
    "    df = df_raw[~((df_raw.index < start_hour) & (df_raw.index.minute == 59)) & (df_raw.index < end_hour)]\n",
    "    \n",
    "    rad2deg = 180.0 / np.pi\n",
    "\n",
    "    # Convert columns to numeric, replacing empty strings with NaN\n",
    "    numeric_cols = ['SonicX', 'SonicY', 'SonicZ', 'SonicT',\n",
    "                   'ShipSonicX', 'ShipSonicY', 'ShipSonicZ', 'ShipSonicT',\n",
    "                   'RotX', 'RotY', 'RotZ', 'AccX', 'AccY', 'AccZ']\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Calculate wind components\n",
    "    result_df = pd.DataFrame(index=df.index)\n",
    "    result_df['u_ms'] = df['SonicY'] / 100.0\n",
    "    result_df['v_ms'] = df['SonicX'] / -100.0\n",
    "    result_df['w_ms'] = df['SonicZ'] / 100.0\n",
    "    result_df['t_degC'] = df['SonicT'] / 100.0\n",
    "    \n",
    "    result_df['u_ms_ship'] = df['ShipSonicY'] / 100.0\n",
    "    result_df['v_ms_ship'] = df['ShipSonicX'] / -100.0\n",
    "    result_df['w_ms_ship'] = df['ShipSonicZ'] / 100.0\n",
    "    result_df['t_degC_ship'] = df['ShipSonicT'] / 100.0\n",
    "\n",
    "    # Rotation and Acceleration\n",
    "    result_df['rotx_degs'] = rad2deg * df['RotX'] / 1000.0\n",
    "    result_df['roty_degs'] = rad2deg * df['RotY'] / 1000.0\n",
    "    result_df['rotz_degs'] = rad2deg * df['RotZ'] / 1000.0\n",
    "    result_df['accelx_g'] = df['AccX'] / -1000.0\n",
    "    result_df['accely_g'] = df['AccY'] / -1000.0\n",
    "    result_df['accelz_g'] = df['AccZ'] / -1000.0\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'df' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m df\u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(data_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mCR6\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m filename, skiprows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, delimiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m2\u001b[39m:]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m df_cr6 \u001b[38;5;241m=\u001b[39m \u001b[43mread_cr6_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 4\u001b[0m, in \u001b[0;36mread_cr6_data\u001b[1;34m(df_raw)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_cr6_data\u001b[39m(df_raw):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Convert TIMESTAMP to datetime and set as index\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     df_raw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTIMESTAMP\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df_raw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTIMESTAMP\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISO8601\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     df_raw \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTIMESTAMP\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m#delete non hourly data\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     start_hour \u001b[38;5;241m=\u001b[39m df_raw\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mfloor(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'df' referenced before assignment"
     ]
    }
   ],
   "source": [
    "for filename  in cr6_name:\n",
    "    df= pd.read_csv(data_dir + '\\\\CR6\\\\' + filename, skiprows=1, delimiter = ',')\n",
    "    df = df.iloc[2:].reset_index(drop=True)\n",
    "    df_cr6 = read_cr6_data(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fluxes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
