{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading instruments SDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os \n",
    "import glob \n",
    "import math \n",
    "import pynmea2\n",
    "import re \n",
    "from scipy import signal\n",
    "%matplotlib widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data directory\n",
    "data_dir = 'C:/Users/ica/OneDrive - Plymouth Marine Laboratory/vscode/EC_co2_flux/EC flux processing/SDA_IMC/data_piccolo_sample'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data directory \n",
    "#data_dir = 'C:/Users/ENV/OneDrive - Plymouth Marine Laboratory/vscode/EC_co2_flux/EC flux processing/SDA_IMC/data_piccolo_sample'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make CR6 files hourly , only need to run once "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_cr6_hourly_data(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Process CR6 data files to group and filter data by hour,\n",
    "    saving output files in a single directory.\n",
    "    \n",
    "    Args:\n",
    "    input_path (str): Directory containing CR6 data files\n",
    "    output_path (str): Directory to save processed hourly files\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Collect all .dat files in the input directory\n",
    "    data_files = [f for f in os.listdir(input_path) if f.endswith('.dat') and f.startswith('CR6data_')]\n",
    "    \n",
    "    # Process each file\n",
    "    for filename in data_files:\n",
    "        try:\n",
    "            # Read the file, skipping the first header row\n",
    "            df_raw = pd.read_csv(\n",
    "                os.path.join(input_path, filename), \n",
    "                skiprows=1, \n",
    "                delimiter=','\n",
    "            )\n",
    "            \n",
    "            # Remove unnecessary rows and reset the index\n",
    "            df_raw = df_raw.iloc[2:].reset_index(drop=True)\n",
    "\n",
    "            # Convert 'TIMESTAMP' column to datetime and set as index\n",
    "            df_raw['TIMESTAMP'] = pd.to_datetime(\n",
    "                df_raw['TIMESTAMP'], \n",
    "                format='%Y-%m-%d %H:%M:%S.%f', \n",
    "                errors=\"coerce\"\n",
    "            )\n",
    "            df_raw = df_raw.set_index('TIMESTAMP')\n",
    "\n",
    "            # Skip if no valid timestamps\n",
    "            if df_raw.empty or df_raw.index.empty:\n",
    "                print(f\"No valid timestamps in {filename}\")\n",
    "                continue\n",
    "\n",
    "            # Define the range for valid hourly data\n",
    "            start_hour = df_raw.index.min().floor('h')\n",
    "            end_hour = df_raw.index.max().ceil('h')\n",
    "\n",
    "            # Filter data to exclude non-hourly records \n",
    "            df_filtered = df_raw.loc[\n",
    "                ~((df_raw.index < start_hour) & (df_raw.index.minute == 59)) & \n",
    "                (df_raw.index <= end_hour)\n",
    "            ].copy()\n",
    "\n",
    "            # Logging and additional checks\n",
    "            print(f\"Processing file: {filename}\")\n",
    "            print(f\"Total records: {len(df_raw)}\")\n",
    "            print(f\"Filtered records: {len(df_filtered)}\")\n",
    "            print(f\"First timestamp: {df_filtered.index.min()}\")\n",
    "            print(f\"Last timestamp: {df_filtered.index.max()}\")\n",
    "\n",
    "            # Skip if no data after filtering\n",
    "            if df_filtered.empty:\n",
    "                print(f\"No valid hourly data in {filename}\")\n",
    "                continue\n",
    "\n",
    "            # Process each unique hour in the filtered data\n",
    "            for hour in df_filtered.index.floor('h').unique():\n",
    "                # Filter data for this specific hour\n",
    "                df_hour = df_filtered.loc[\n",
    "                    (df_filtered.index >= hour) & \n",
    "                    (df_filtered.index < hour + timedelta(hours=1))\n",
    "                ]\n",
    "\n",
    "                # Create output filename using the hour\n",
    "                output_filename = f\"CR6_Processed_{hour.strftime('%Y%m%d_%H%M%S')}.dat\"\n",
    "                output_file = os.path.join(output_path, output_filename)\n",
    "\n",
    "                # Prepare column names based on the original header\n",
    "                column_names = [\n",
    "                    'SonicX', 'SonicY', 'SonicZ', 'SonicT', \n",
    "                    'RotX', 'RotY', 'RotZ', \n",
    "                    'AccX', 'AccY', 'AccZ', \n",
    "                    'ShipSonicX', 'ShipSonicY', 'ShipSonicZ', 'ShipSonicT'\n",
    "                ]\n",
    "\n",
    "                # Reset index to include timestamp as a column\n",
    "                df_output = df_hour.reset_index()\n",
    "                \n",
    "                # Ensure we only keep specified columns (or use all columns if not enough specified)\n",
    "                columns_to_keep = ['TIMESTAMP'] + column_names[:len(df_output.columns)-1]\n",
    "                df_output = df_output[columns_to_keep]\n",
    "\n",
    "                # Write processed data\n",
    "                df_output.to_csv(\n",
    "                    output_file, \n",
    "                    sep='\\t', \n",
    "                    index=False, \n",
    "                    \n",
    "                    na_rep='NaN'\n",
    "                )\n",
    "                \n",
    "                print(f\"Processed hourly data to {output_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_base_path = 'C:/Users/ica/OneDrive - Plymouth Marine Laboratory/vscode/EC_co2_flux/EC flux processing/SDA_IMC/data_piccolo_sample/CR6'\n",
    "output_base_path = 'C:/Users/ica/OneDrive - Plymouth Marine Laboratory/vscode/EC_co2_flux/EC flux processing/SDA_IMC/data_piccolo_sample/CR6_hourly'\n",
    "process_cr6_hourly_data(input_base_path, output_base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make picarro files hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def process_hourly_data(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Process data files to ensure they are named and filtered for the correct hour,\n",
    "    saving all output files in a single flat directory.\n",
    "    \n",
    "    Args:\n",
    "    input_path (str): Root directory containing year/month/day folder structure\n",
    "    output_path (str): Directory to save processed files\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Dictionary to group files by their actual data hour\n",
    "    hourly_files = {}\n",
    "    \n",
    "    # Walk through the directory structure to collect files\n",
    "    for root, dirs, files in os.walk(input_path):\n",
    "        for file in files:\n",
    "            # Check if file is a data log file\n",
    "            if file.endswith('.dat') and 'DataLog_User_Sync' in file:\n",
    "                # Construct full input file path\n",
    "                input_file = os.path.join(root, file)\n",
    "                \n",
    "                # Read the data file\n",
    "                try:\n",
    "                    # Read file with updated parsing\n",
    "                    df = pd.read_csv(input_file, sep='\\s+', header=0)\n",
    "                    \n",
    "                    # Combine DATE and TIME columns into a datetime\n",
    "                    df['datetime'] = pd.to_datetime(df['DATE'] + ' ' + df['TIME'])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {input_file}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                # Determine the actual hour of the data (based on the majority of timestamps)\n",
    "                most_common_hour = df['datetime'].dt.floor('h').mode()[0]\n",
    "                \n",
    "                # Store files in the hourly_files dictionary\n",
    "                if most_common_hour not in hourly_files:\n",
    "                    hourly_files[most_common_hour] = []\n",
    "                hourly_files[most_common_hour].append(input_file)\n",
    "    \n",
    "    # Process each hour's worth of files\n",
    "    for data_hour, files in hourly_files.items():\n",
    "        # Collect and concatenate dataframes for this hour\n",
    "        dfs = []\n",
    "        for input_file in files:\n",
    "            try:\n",
    "                # Read file with updated parsing\n",
    "                df = pd.read_csv(input_file, sep='\\s+', header=0)\n",
    "                \n",
    "                # Combine DATE and TIME columns into a datetime\n",
    "                df['datetime'] = pd.to_datetime(df['DATE'] + ' ' + df['TIME'])\n",
    "                dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {input_file}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Combine all dataframes for this hour\n",
    "        if not dfs:\n",
    "            continue\n",
    "        \n",
    "        df_combined = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "        # Precise filtering to include only rows for the specific hour\n",
    "        df_filtered = df_combined[\n",
    "            (df_combined['datetime'] >= data_hour) & \n",
    "            (df_combined['datetime'] < data_hour + timedelta(hours=1))\n",
    "        ].sort_values('datetime')\n",
    "        \n",
    "        # If no data in the hour, skip\n",
    "        if df_filtered.empty:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # Create output filename using the actual data hour #potential issue if eod or end of month\n",
    "        output_filename = f\"CFHADS2041-{data_hour.strftime('%Y%m%d')}-{data_hour.strftime('%H%M%S')}Z-DataLog_User_Sync.dat\"\n",
    "        \n",
    "        # Full output path (now in a single flat directory)\n",
    "        output_file = os.path.join(output_path, output_filename)\n",
    "        \n",
    "        # Prepare columns for output\n",
    "        output_columns = [\n",
    "            'datetime', 'FRAC_DAYS_SINCE_JAN1', 'FRAC_HRS_SINCE_JAN1', \n",
    "            'JULIAN_DAYS', 'EPOCH_TIME', 'ALARM_STATUS', 'INST_STATUS', \n",
    "            'CavityPressure_sync', 'CavityTemp_sync', 'DasTemp_sync', \n",
    "            'EtalonTemp_sync', 'WarmBoxTemp_sync', 'species_sync', \n",
    "            'MPVPosition_sync', 'OutletValve_sync', 'solenoid_valves_sync', \n",
    "            'CH4_sync', 'CH4_dry_sync', 'CO2_sync', 'CO2_dry_sync', 'H2O_sync'\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        # Write processed data\n",
    "        df_output = df_filtered[output_columns]\n",
    "        df_output.to_csv(output_file, sep='\\t', \n",
    "                         index=False, \n",
    "                        )\n",
    "        \n",
    "        print(f\"Processed hourly data to {output_file}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_base_path = 'C:/Users/ica/OneDrive - Plymouth Marine Laboratory/vscode/EC_co2_flux/EC flux processing/SDA_IMC/data_piccolo_sample/Picarro'\n",
    "output_base_path = 'C:/Users/ica/OneDrive - Plymouth Marine Laboratory/vscode/EC_co2_flux/EC flux processing/SDA_IMC/data_piccolo_sample/picarro_hourly'\n",
    "process_hourly_data(input_base_path, output_base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run at the start "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get filenames in raw data files\n",
    "\n",
    "def file_name_dir(data_dir):\n",
    "    \"\"\"\n",
    "    get the raw EC data file names and directories\n",
    "\n",
    "    input:  the parent directory of the raw EC data\n",
    "    return: the EC data file names (_name) and directories(_dir)\n",
    "    \n",
    "    \"\"\"    \n",
    "\n",
    "    cr6_name = os.listdir(data_dir + '\\\\CR6_hourly')\n",
    "    GPS_name = [f for f in os.listdir(data_dir + '\\\\Underway') if 'gps' in f]\n",
    "    HDG_name = [f for f in os.listdir(data_dir + '\\\\Underway') if 'hdg' in f]\n",
    "    metek_name = os.listdir(data_dir + '\\\\Metek')\n",
    "    cr800_name = os.listdir(data_dir + '\\\\CR800')\n",
    "    Picarro_name = os.listdir(data_dir + '\\\\picarro_hourly')\n",
    "\n",
    " \n",
    "    return cr6_name,GPS_name,HDG_name, metek_name,cr800_name,Picarro_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates lists of files to be processed \n",
    "cr6_name,GPS_name, HDG_name,metek_name,cr800_name,Picarro_name = file_name_dir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract timestamp from filename\n",
    "def extract_timestamp(filename):\n",
    "    \"\"\"Extract timestamp from different filename formats and return a datetime object.\"\"\"\n",
    "    patterns = [\n",
    "        r\"(\\d{4}-\\d{2}-\\d{2}T\\d{2}-\\d{2}-\\d{2}Z)\",  # Format: 2024-01-30T01-00-00Z\n",
    "        r\"(\\d{8}_\\d{6})\",  # Format: CR6_Processed_20240130_010000.dat\n",
    "        r\"(\\d{8}\\d{4})\",  # Format: 202401300100COM2.txt\n",
    "        r\"(\\d{4}_\\d{2}_\\d{2}_\\d{4})\",  # Format: Waterwatcher_2024_01_30_0600.dat\n",
    "        r\"(\\d{8}-\\d{6}Z)\"  # **Picarro format: YYYYMMDD-HHMMSSZ**\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, filename)\n",
    "        if match:\n",
    "            ts_str = match.group(1)\n",
    "\n",
    "            # Define matching formats for each pattern\n",
    "            formats = [\n",
    "                \"%Y-%m-%dT%H-%M-%SZ\",  # 2024-01-30T01-00-00Z\n",
    "                \"%Y%m%d_%H%M%S\",  # 20240130_010000\n",
    "                \"%Y%m%d%H%M\",  # 202401300100\n",
    "                \"%Y_%m_%d_%H%M\",  # 2024_01_30_0600\n",
    "                \"%Y%m%d-%H%M%SZ\"  # **Picarro format: 20240130-010000Z**\n",
    "            ]\n",
    "\n",
    "            for fmt in formats:\n",
    "                try:\n",
    "                    return datetime.strptime(ts_str, fmt)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make new folder to store the processed data\n",
    "new_folder_path = os.path.join(data_dir, \"L0_test\")\n",
    "\n",
    "# Create the new folder\n",
    "os.makedirs(new_folder_path, exist_ok=True)\n",
    "\n",
    "# subfolders to create\n",
    "subfolders = [\"TimeAdjGases\", \"PML_WindsForMotcorr\", \"Ship_WindsForMotcorr\"]\n",
    "\n",
    "# subfolder inside \"L0\"\n",
    "for subfolder in subfolders:\n",
    "    os.makedirs(os.path.join(new_folder_path, subfolder), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to read raw data files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read underway.gps files\n",
    "def read_gps_data(filename):\n",
    "    \"\"\"\n",
    "    Reads an NMEA file and extracts datetime, latitude, longitude, and speed.\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        'datetime': [],\n",
    "        'latitude': [],\n",
    "        'longitude': [],\n",
    "        'speed': []\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(data_dir, 'Underway', filename)) as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                # Split timestamp from NMEA sentence\n",
    "                timestamp_str, nmea = line.strip().split(' $')\n",
    "                nmea = '$' + nmea\n",
    "                timestamp = pd.to_datetime(timestamp_str)\n",
    "                \n",
    "                if \"INVTG\" in nmea:\n",
    "                    msg = pynmea2.parse(nmea)\n",
    "                    if msg.spd_over_grnd_kts:\n",
    "                        speed = float(msg.spd_over_grnd_kts)\n",
    "                        data['datetime'].append(timestamp)\n",
    "                        data['speed'].append(speed)\n",
    "                        data['latitude'].append(None)\n",
    "                        data['longitude'].append(None)\n",
    "                        \n",
    "                elif \"INGGA\" in nmea:\n",
    "                    msg = pynmea2.parse(nmea)\n",
    "                    if msg.latitude and msg.longitude:\n",
    "                        lat = round(msg.latitude,6)\n",
    "                        lon = round(msg.longitude,6)\n",
    "                        if msg.lat_dir == 'S':\n",
    "                            lat = -lat\n",
    "                        if msg.lon_dir == 'W':\n",
    "                            lon = -lon\n",
    "                        data['datetime'].append(timestamp)\n",
    "                        data['latitude'].append(lat)\n",
    "                        data['longitude'].append(lon)\n",
    "                        data['speed'].append(None)\n",
    "                        \n",
    "            except (ValueError, pynmea2.ParseError) as e:\n",
    "                continue\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.set_index('datetime')  # Assign the result back to df\n",
    "    df.index = df.index.tz_localize(None)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read underway.hdg.txt\n",
    "def read_hdg_data(filename):\n",
    "    data = {\n",
    "        'datetime': [],\n",
    "        'heading': [],\n",
    "        'rate_of_turn': []\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(data_dir, 'Underway', filename)) as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                timestamp_str, nmea = line.strip().split(' $')\n",
    "                nmea = '$' + nmea\n",
    "                timestamp = pd.to_datetime(timestamp_str)\n",
    "                \n",
    "                if \"INHDT\" in nmea:\n",
    "                    msg = pynmea2.parse(nmea)\n",
    "                    if msg.heading:\n",
    "                        heading = float(msg.heading)\n",
    "                        data['datetime'].append(timestamp)\n",
    "                        data['heading'].append(heading)\n",
    "                        data['rate_of_turn'].append(None)\n",
    "                      \n",
    "                elif \"INROT\" in nmea:\n",
    "                    msg = pynmea2.parse(nmea)\n",
    "                    if msg.rate_of_turn:\n",
    "                        rot = float(msg.rate_of_turn)\n",
    "                        data['datetime'].append(timestamp)\n",
    "                        data['heading'].append(None)\n",
    "                        data['rate_of_turn'].append(rot)\n",
    "                \n",
    "                        \n",
    "            except (ValueError, pynmea2.ParseError) as e:\n",
    "                continue\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.set_index('datetime')  # Assign the result back to df\n",
    "    df.index = df.index.tz_localize(None)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wind data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cr6_data(filename):\n",
    "    df = pd.read_csv(os.path.join(data_dir, 'CR6_hourly', filename),  delimiter='\\t')\n",
    "    df = df.iloc[2:].reset_index(drop=True)\n",
    "    # Convert TIMESTAMP to datetime and set as index\n",
    "    df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'], format='%Y-%m-%d %H:%M:%S.%f', errors=\"coerce\")\n",
    "    df = df.set_index('TIMESTAMP')\n",
    "\n",
    "    rad2deg = 180.0 / np.pi\n",
    "\n",
    "    # Convert columns to numeric, replacing empty strings with NaN\n",
    "    numeric_cols = ['SonicX', 'SonicY', 'SonicZ', 'SonicT',\n",
    "                    'ShipSonicX', 'ShipSonicY', 'ShipSonicZ', 'ShipSonicT',\n",
    "                    'RotX', 'RotY', 'RotZ', 'AccX', 'AccY', 'AccZ']\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        df.loc[:, col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Calculate wind components\n",
    "    result_df = pd.DataFrame(index=df.index)\n",
    "    result_df['u_ms'] = df['SonicY'] / 100.0\n",
    "    result_df['v_ms'] = df['SonicX'] / -100.0\n",
    "    result_df['w_ms'] = df['SonicZ'] / 100.0\n",
    "    result_df['t_degC'] = df['SonicT'] / 100.0\n",
    "    \n",
    "    result_df['u_ms_ship'] = df['ShipSonicY'] / 100.0\n",
    "    result_df['v_ms_ship'] = df['ShipSonicX'] / -100.0\n",
    "    result_df['w_ms_ship'] = df['ShipSonicZ'] / 100.0\n",
    "    result_df['t_degC_ship'] = df['ShipSonicT'] / 100.0\n",
    "\n",
    "    # Rotation and Acceleration\n",
    "    result_df['rotx_degs'] = rad2deg * df['RotX'] / 1000.0\n",
    "    result_df['roty_degs'] = rad2deg * df['RotY'] / 1000.0\n",
    "    result_df['rotz_degs'] = rad2deg * df['RotZ'] / 1000.0\n",
    "    result_df['accelx_g'] = df['AccX'] / -1000.0\n",
    "    result_df['accely_g'] = df['AccY'] / -1000.0\n",
    "    result_df['accelz_g'] = df['AccZ'] / -1000.0\n",
    "    \n",
    "    def despike_column(series, sigma_threshold=5):\n",
    "        \"\"\"\n",
    "        Remove spikes from a time series using sigma-based method\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        series : pandas.Series\n",
    "            Input time series to despike\n",
    "        sigma_threshold : float, optional\n",
    "            Number of standard deviations to use as threshold (default: 5)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.Series\n",
    "            Despiked time series with spikes replaced by NaN\n",
    "        \"\"\"\n",
    "        # Calculate mean and standard deviation\n",
    "        v_avg = series.mean()\n",
    "        v_sdev = series.std()\n",
    "        \n",
    "        # Create a mask for non-spike values\n",
    "        mask = np.abs(series - v_avg) < sigma_threshold * v_sdev\n",
    "        \n",
    "        # Return series with spikes replaced by NaN\n",
    "        return series * mask\n",
    "    \n",
    "    # Apply despiking to relevant columns\n",
    "    despike_columns = [\n",
    "        'u_ms', 'v_ms', 'w_ms', 't_degC', \n",
    "        'u_ms_ship', 'v_ms_ship', 'w_ms_ship', 't_degC_ship',\n",
    "        'rotx_degs', 'roty_degs', 'rotz_degs', \n",
    "        'accelx_g', 'accely_g', 'accelz_g'\n",
    "    ]\n",
    "    \n",
    "    for col in despike_columns:\n",
    "        result_df[col] = despike_column(result_df[col])\n",
    "\n",
    "    return result_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Water watcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cr800_data(filename):\n",
    "    df = pd.read_csv(os.path.join(data_dir, 'cr800', filename), skiprows=1, delimiter=',')\n",
    "    df = df.iloc[2:].reset_index(drop=True)\n",
    "    # Convert TIMESTAMP to datetime and set as index\n",
    "    df[\"TIMESTAMP\"] = pd.to_datetime(df[\"TIMESTAMP\"], format='mixed')\n",
    "    # Convert all non-datetime columns to float\n",
    "    for col in df.columns:\n",
    "        if col != \"TIMESTAMP\":\n",
    "            df[col] = pd.to_numeric(df[col])\n",
    "    df = df.set_index(\"TIMESTAMP\")  # Set the timestamp as index\n",
    "    df = df.iloc[:,3:]  # Select columns after index 3\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_metek_data (filename):\n",
    "\n",
    "    # Regex pattern to extract valid lines with x, y, z, and t\n",
    "    pattern = re.compile(\n",
    "        r'(?P<datetime>\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d{6}Z)\\s+H:x =\\s+(?P<x>-?\\d+)\\s+y =\\s+(?P<y>-?\\d+)\\s+z =\\s+(?P<z>-?\\d+)\\s+t =\\s+(?P<t>-?\\d+)'\n",
    "    )\n",
    "\n",
    "    # Read and process lines\n",
    "    data = []\n",
    "    with open(os.path.join(data_dir, 'Metek', filename)) as file:\n",
    "        for line in file:\n",
    "            match = pattern.search(line)\n",
    "            if match:\n",
    "                data.append(match.groupdict())\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Convert data types\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])  \n",
    "    df[\"x\"] = df[\"x\"].astype(float)\n",
    "    df[\"y\"] = df[\"y\"].astype(float)\n",
    "    df[\"z\"] = df[\"z\"].astype(float)\n",
    "    df[\"t\"] = df[\"t\"].astype(float)\n",
    "\n",
    "    df[\"x\"] = df[\"x\"]/ -100.0\n",
    "    df[\"y\"] = df[\"y\"]/ 100.0\n",
    "    df[\"z\"] = df[\"z\"] / 100.0\n",
    "    df[\"t\"] = df[\"t\"]/ 100.0\n",
    "\n",
    "\n",
    "    # Set datetime as index\n",
    "    df.set_index(\"datetime\", inplace=True)  \n",
    "    df.index = df.index.tz_localize(None)\n",
    "    \n",
    "\n",
    "    def despike_column(series, sigma_threshold=5):\n",
    "            \"\"\"\n",
    "            Remove spikes from a time series using sigma-based method\n",
    "            \n",
    "            Parameters:\n",
    "            -----------\n",
    "            series : pandas.Series\n",
    "                Input time series to despike\n",
    "            sigma_threshold : float, optional\n",
    "                Number of standard deviations to use as threshold (default: 5)\n",
    "            \n",
    "            Returns:\n",
    "            --------\n",
    "            pandas.Series\n",
    "                Despiked time series with spikes replaced by NaN\n",
    "            \"\"\"\n",
    "            # Calculate mean and standard deviation\n",
    "            v_avg = series.mean()\n",
    "            v_sdev = series.std()\n",
    "            \n",
    "            # Create a mask for non-spike values\n",
    "            mask = np.abs(series - v_avg) < sigma_threshold * v_sdev\n",
    "            \n",
    "            # Return series with spikes replaced by NaN\n",
    "            return series * mask\n",
    "        \n",
    "        # Apply despiking to relevant columns\n",
    "    despike_columns = [\n",
    "            'x', 'y', 'z', 't']\n",
    "            \n",
    "        \n",
    "    for col in despike_columns:\n",
    "        df[col] = despike_column(df[col])\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picarro data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_picarro_data(filename):\n",
    "    df = pd.read_csv(os.path.join(data_dir, 'picarro_hourly', filename), sep='\\t')\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "\n",
    "    # drop date and time columns\n",
    "    df.set_index('datetime', inplace=True) \n",
    "    #add true and false value \n",
    "    #df_picarro['puff'] = df_picarro['solenoid_valve'] != 0 in data it is 32 fro some reason?     \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once there is a df for each hourly file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lag time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_time(df_cr6, df_metek, cr6_col='v_ms', metek_col='x', max_lag_seconds=300):\n",
    "    \"\"\"\n",
    "    Calculate the lag time between two time series using maximum covariance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_cr6 : pandas.DataFrame\n",
    "        DataFrame with datetime index containing CR6 data\n",
    "    df_metek : pandas.DataFrame\n",
    "        DataFrame with datetime index containing Metek data\n",
    "    cr6_col : str, optional\n",
    "        Column name in df_cr6 to use for comparison (default: 'v_ms')\n",
    "    metek_col : str, optional\n",
    "        Column name in df_metek to use for comparison (default: 'x')\n",
    "    max_lag_seconds : int, optional\n",
    "        Maximum lag time to consider in seconds (default: 300)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    lag_seconds : float\n",
    "        Lag time in seconds. Positive means df_metek lags behind df_cr6,\n",
    "        negative means df_cr6 lags behind df_metek.\n",
    "    max_covariance : float\n",
    "        Maximum covariance value\n",
    "    \"\"\"\n",
    "    # Extract the columns of interest\n",
    "    cr6_series = df_cr6[cr6_col].copy()\n",
    "    metek_series = df_metek[metek_col].copy()\n",
    "    \n",
    "    # Ensure both series are on the same frequency\n",
    "    # Determine the time difference between consecutive timestamps\n",
    "    cr6_freq = pd.Series(cr6_series.index[1:] - cr6_series.index[:-1]).median()\n",
    "    metek_freq = pd.Series(metek_series.index[1:] - metek_series.index[:-1]).median()\n",
    "    \n",
    "    # Use the smaller frequency for resampling\n",
    "    common_freq = min(cr6_freq, metek_freq)\n",
    "    common_freq_str = f\"{common_freq.total_seconds()}s\"\n",
    "    \n",
    "    # Resample to common frequency\n",
    "    cr6_resampled = cr6_series.resample(common_freq_str).mean()\n",
    "    metek_resampled = metek_series.resample(common_freq_str).mean()\n",
    "    \n",
    "    # Get overlapping time range\n",
    "    start_time = max(cr6_resampled.index.min(), metek_resampled.index.min())\n",
    "    end_time = min(cr6_resampled.index.max(), metek_resampled.index.max())\n",
    "    \n",
    "    # Filter data to overlapping period\n",
    "    cr6_filtered = cr6_resampled[start_time:end_time].interpolate()\n",
    "    metek_filtered = metek_resampled[start_time:end_time].interpolate()\n",
    "    \n",
    "    # Calculate maximum number of lags based on specified max lag\n",
    "    lag_points = int(max_lag_seconds / common_freq.total_seconds())\n",
    "    \n",
    "    # Calculate cross-correlation\n",
    "    x = cr6_filtered.values - np.mean(cr6_filtered.values)\n",
    "    y = metek_filtered.values - np.mean(metek_filtered.values)\n",
    "    \n",
    "    # Remove any NaN values\n",
    " \n",
    "    \n",
    "    correlation = signal.correlate(x, y, mode='full')\n",
    "    lags = signal.correlation_lags(len(x), len(y), mode='full')\n",
    "    \n",
    "    # Find index of maximum correlation\n",
    "    max_idx = np.argmax(np.abs(correlation))\n",
    "    max_lag = lags[max_idx]\n",
    "    \n",
    "    # Convert lag from points to seconds\n",
    "    lag_seconds = max_lag * common_freq.total_seconds()\n",
    "    max_covariance = correlation[max_idx]\n",
    "    \n",
    "    return lag_seconds, max_covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flag water watcher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def water_watcher_flag(df_water_watcher, df_picarro):\n",
    "    \"\"\"\n",
    "    Add a boolean column to the Picarro dataframe indicating whether the water watcher\n",
    "    was on at each timestamp.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_water_watcher : pandas.DataFrame\n",
    "        DataFrame with datetime index and WaterWatcherState_Avg column\n",
    "    df_picarro : pandas.DataFrame\n",
    "        DataFrame with datetime index to which the flag column will be added\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    df_picarro : pandas.DataFrame\n",
    "        The Picarro DataFrame with an added 'water_watcher_on' column\n",
    "    \"\"\"\n",
    "    # Create a copy of the Picarro dataframe to avoid modifying the original\n",
    "    df_result = df_picarro.copy()\n",
    "    '''\n",
    "    # Make sure both dataframes have datetime indices\n",
    "    if not isinstance(df_water_watcher.index, pd.DatetimeIndex):\n",
    "        df_water_watcher.index = pd.to_datetime(df_water_watcher.index)\n",
    "    \n",
    "    if not isinstance(df_result.index, pd.DatetimeIndex):\n",
    "        df_result.index = pd.to_datetime(df_result.index)\n",
    "    '''\n",
    "    # Create a new Series with the water watcher state\n",
    "    # Assuming any non-zero value means \"on\"\n",
    "    water_watcher_values = pd.to_numeric(df_water_watcher['Valve_Status'], errors='coerce')\n",
    "    water_watcher_series = (water_watcher_values > 0) & ~pd.isna(water_watcher_values)\n",
    "    \n",
    "    # Reindex the water watcher series to match Picarro timestamps\n",
    "    water_watcher_reindexed = water_watcher_series.reindex(\n",
    "        df_result.index, \n",
    "        method='nearest',\n",
    "        tolerance=pd.Timedelta('1s')  # Increased tolerance to 1 second\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Add the new column to the Picarro dataframe\n",
    "    df_result['water_watcher_on'] = water_watcher_reindexed\n",
    "\n",
    "    \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Angle correction for metek "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the files are ready, start processing them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(len(cr6_name)):\n",
    "        \n",
    "        # Extract timestamps\n",
    "        timestamps = {\n",
    "            \"CR6\": extract_timestamp(cr6_name[i]),\n",
    "            \"GPS\": extract_timestamp(GPS_name[i]),\n",
    "            \"HDG\": extract_timestamp(HDG_name[i]),\n",
    "            \"CR800\": extract_timestamp(cr800_name[i]),\n",
    "            \"Picarro\": extract_timestamp(Picarro_name[i])\n",
    "        }\n",
    "        \n",
    "        # Check if all timestamps are the same\n",
    "        if len(set(timestamps.values())) > 1:\n",
    "            print(f\"Timestamp mismatch at index {i}: {timestamps}\")\n",
    "            continue  # Skip this iteration\n",
    "        \n",
    "        # Read the data if timestamps match\n",
    "        df_cr6 = read_cr6_data(os.path.join(data_dir, 'CR6_hourly', cr6_name[i]))\n",
    "        df_gps = read_gps_data(os.path.join(data_dir, 'Underway', GPS_name[i]))\n",
    "        df_hdg = read_hdg_data(os.path.join(data_dir, 'Underway', HDG_name[i]))\n",
    "        df_metek = read_metek_data(os.path.join(data_dir, 'Metek', metek_name[i]))\n",
    "        df_picarro = read_picarro_data(os.path.join(data_dir, 'picarro_hourly', Picarro_name[i]))\n",
    "        df_water_watcher = read_cr800_data(os.path.join(data_dir, 'cr800', cr800_name[i]))\n",
    "        df_underway = pd.merge(df_gps, df_hdg, left_index=True, right_index=True, how=\"outer\")\n",
    "\n",
    "        #lagtime\n",
    "        lag_seconds, max_cov = lag_time(df_cr6, df_metek, cr6_col='v_ms', metek_col='x')\n",
    "        print(f\"Lag time: {lag_seconds} seconds\")\n",
    "        df_picarro_lag = df_picarro.copy()\n",
    "        df_picarro_lag.index = df_picarro_lag.index + pd.Timedelta(seconds=lag_seconds)\n",
    "\n",
    "        #water watcher flag\n",
    "        df_picarro_flag = water_watcher_flag(df_water_watcher, df_picarro_lag)\n",
    "\n",
    "        \n",
    "        break\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def correct_wind_direction(df_cr6):\n",
    "    # Split df_cr6 into PML and ship sonic dataframes\n",
    "    df_pml_sonic = df_cr6[['u_ms', 'v_ms', 'w_ms', 't_degC']].copy()\n",
    "    df_ship_sonic = df_cr6[['u_ms_ship', 'v_ms_ship', 'w_ms_ship', 't_degC_ship']].copy()\n",
    "\n",
    "    # Drop rows with NaN values in the relevant columns for each dataframe\n",
    "    df_pml_sonic = df_pml_sonic.dropna(subset=['u_ms', 'v_ms'])\n",
    "    df_ship_sonic = df_ship_sonic.dropna(subset=['u_ms_ship', 'v_ms_ship'])\n",
    "\n",
    "    df_pml_sonic[['u_ms', 'v_ms']] = df_pml_sonic[['u_ms', 'v_ms']].astype(float)\n",
    "    df_ship_sonic[['u_ms_ship', 'v_ms_ship']] = df_ship_sonic[['u_ms_ship', 'v_ms_ship']].astype(float)\n",
    "\n",
    "    \n",
    "    # Calculate wind direction for both sonics (in degrees)\n",
    "    df_pml_sonic['wd'] = (270 - np.degrees(np.arctan2(df_pml_sonic['v_ms'], df_pml_sonic['u_ms']))) % 360\n",
    "    df_ship_sonic['wd'] = (270 - np.degrees(np.arctan2(df_ship_sonic['v_ms_ship'], df_ship_sonic['u_ms_ship']))) % 360\n",
    "\n",
    "    # Calculate mean wind direction difference\n",
    "    wd_diff = df_ship_sonic['wd'].mean() - df_pml_sonic['wd'].mean()\n",
    "\n",
    "\n",
    "\n",
    "    return wd_diff, df_pml_sonic, df_ship_sonic\n",
    "\n",
    "# Test the function\n",
    "wd_diff, df_pml_sonic, df_ship_sonic = correct_wind_direction(df_cr6)\n",
    "print(wd_diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(df_pml_sonic['wd'], label='PML')\n",
    "plt.plot(df_ship_sonic['wd'],  label='SHIP')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fluxes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
